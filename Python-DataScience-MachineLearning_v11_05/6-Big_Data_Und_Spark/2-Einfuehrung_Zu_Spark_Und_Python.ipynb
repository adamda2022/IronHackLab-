{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://akademie.datamics.com/kursliste/\">![title](bg_datamics_top.png)</a><center><em>© Datamics</em></center><br><center><em>Besuche uns für mehr Informationen auf <a href='https://akademie.datamics.com/kursliste/'>www.akademie.datamics.com</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung zu Spark und Python\n",
    "\n",
    "Wir können jetzt lernen, wie wir Spark mit Python verwenden können, indem wir die `pyskark` Library verwenden. Gehe sicher, dass du die Video-Lektionen zu Spark und RDDs anschaust, bevor du mit diesem Code anfängst.\n",
    "\n",
    "Dieses Notebook dient als Code Refrenz für den Big Data Abschnitt des Kurses, der Amazon Web Services einschließt. Das Video wird ausführlichere Erklärungen zum Code bieten.\n",
    "\n",
    "## Einen SparkContext erstellen\n",
    "\n",
    "Zuerst müssen wir einen `SparkContext` erstellen. Wir importieren ihn aus `pyspark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt erstellen wir einen SparkContext. Ein SparkContext repräsentiert eine Verbinding zu einem Spark Cluster. Es kann dazu verwendet werden ein RDD zu erstellen und Variablen an das Cluster zu übergeben.\n",
    "\n",
    "*Hinweis: Du kannst nur einen SparkContext auf einmal haben.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlegende Operationen\n",
    "\n",
    "Wir werden mit dem \"Hallo Welt\" Beispiel beginnen. Es besteht darin eine Textdatei zu lesen. Zuerst sollten wir dazu eine solche Datei schreiben.\n",
    "\n",
    "Wir schreiben eine beispielhafte Textdatei, die wir anschließend lesen können. Wir verwenden dazu Jupyter Notebook Code, aber jede andere .txt-Datei kann verwendet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile beispiel.txt\n",
    "erste zeile\n",
    "zweite zeile\n",
    "dritte zeile\n",
    "vierte zeile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD erstellen\n",
    "\n",
    "Jetzt können wir die Textdatei verwenden, indem wir die `textFile` Methode des bereits erstellten SparkContext nutzen. Diese Methode wird eine TextDatei vom HDFS lesen, einem lokalen Dateisystem (verfügbar auf allen Nodes), oder von jeder Hadoop-unterstützten Datei System URI, und gibt es als RDD von Strings zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile('beispiel.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparks primäre Abstraktion ist eine verteilte Sammlung von Items, die *Resilient Distributed Dataset* (RDD) genannt werden. RDDs können durch Hadoop InputFormats (wie bspw. HDFS Dateien) erstellt werden oder durch Transformation anderer RDDs.\n",
    "\n",
    "### Actions\n",
    "\n",
    "Wir haben gereade ein RDD erstellt, indem wir die `textFile` Methode verwendet haben. Auf dieses Objekt können wir jetzt Aktionen anwenden. Zu den möglichen Operationen zählt beispielsweise ein *Count* der Zeilen.\n",
    "\n",
    "RDDs bieten Aktionen (en. Actions), die Werte zurückgeben, und Transformationen, die auf neue RDDs verweisen. Beginnen wir mit zwei Actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Jetzt können wir Transformationen (en. Transformations) verwenden. Ein Beispiel ist die `filter` Transformation, die ein neues RDD zurückgibt, welches ein Subset der urspünglichen Daten beinhaltet. Erstellen wir eine einfache Transformation, indem wir die `filter()` Methode verwenden. Diese Methode (genau wie Python's eigene Filter Funktion) gibt nur Elemente zurück, die die Bedingung erfüllen. Wir können nach Zeilen suchen, in denen \"zwei\" enthalten ist. In unserem Fall sollte es nur eine solche Zeile geben. Überprüfen wir das:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zweifind = textFile.filter(lambda line: 'zwei' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zweifind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action auf die Transformation anwenden\n",
    "zweifind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action auf die Transformation anwenden\n",
    "zweifind.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achte darauf, wie die Transformation keinen Output erzeugt, wenn keine Action darauf ausgeführt wird. In der nächsten Lektion: Fotgeschrittene Spark und Python Inhalte. Wir werden damit beginnen viele weitere Beispiele für Actions, Transformations und deren Zusammenspiel zu sehen.\n",
    "\n",
    "# Gut gemacht!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
